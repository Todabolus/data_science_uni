{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# **Data Science Projekt - Datenvorbereitung**\n",
    "\n",
    "## **Überblick**\n",
    "Dieses Notebook führt die Datenvorbereitung für unser Data Science Projekt durch. Es umfasst:\n",
    "\n",
    "1. **Datenimport und -vereinheitlichung** von News-CSV-Dateien\n",
    "2. **Zeitstempel-Verarbeitung** und Kategorisierung\n",
    "3. **Datenaggregation** für verschiedene Metriken\n",
    "4. **Zusammenführung** mit Chart-Daten\n",
    "5. **Export** der finalen Datei\n",
    "\n",
    "## **Datenquellen**\n",
    "- **News-Daten**: CSV-Dateien aus `data/raw/news/` (verschiedene Kategorien)\n",
    "- **Chart-Daten**: `data/raw/chart/raw_chart_data.csv`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "##  **1. Setup und Imports**\n",
    "\n",
    "Laden der erforderlichen Bibliotheken und Definition von Konstanten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libs\n",
    "from typing import Dict\n",
    "\n",
    "# data science libs\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# import utility functions\n",
    "from utils.data_prep_utils import (\n",
    "    load_and_process_news_data,\n",
    "    aggregate_news_data,\n",
    "    analyze_column_quality,\n",
    "    process_chart_data,\n",
    "    merge_and_finalize_data,\n",
    "    print_final_summary\n",
    ")\n",
    "\n",
    "# configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# constants\n",
    "CSV_FOLDER = \"data/raw/news/\"\n",
    "CHART_DATA_PATH = \"data/raw/chart/raw_chart_data.csv\"\n",
    "OUTPUT_PATH = \"data/merged/merged_by_timestamp.csv\"\n",
    "\n",
    "# Mapping for impact levels\n",
    "IMPACT_MAPPING: Dict[str, int] = {\n",
    "    \"NONE\": 0,\n",
    "    \"LOW\": 1, \n",
    "    \"MEDIUM\": 2,\n",
    "    \"HIGH\": 3\n",
    "}\n",
    "\n",
    "print(\"Setup completed\")\n",
    "print(f\"News data folder: {CSV_FOLDER}\")\n",
    "print(f\"Chart data path: {CHART_DATA_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## **2. Daten einlesen**\n",
    "\n",
    "### Ziel\n",
    "Laden aller News-CSV-Dateien aus verschiedenen Kategorien und Vereinheitlichung der Datenstruktur.\n",
    "\n",
    "### Prozess\n",
    "1. **Dateien scannen**: Alle CSV-Dateien im News-Ordner finden\n",
    "2. **Zeitstempel konvertieren**: String-Format in DateTime umwandeln\n",
    "3. **Impact-Mapping**: Kategorische Impact-Werte in numerische Werte umwandeln\n",
    "4. **Kategorisierung**: Dateiname als Kategorie hinzufügen\n",
    "5. **Zusammenführung**: Alle DataFrames zu einem Master-DataFrame kombinieren\n",
    "6. **Sortierung**: Chronologische Anordnung nach Zeitstempel\n",
    "\n",
    "### Impact-Mapping\n",
    "- `NONE` → 0 (Kein Einfluss)\n",
    "- `LOW` → 1 (Geringer Einfluss)  \n",
    "- `MEDIUM` → 2 (Mittlerer Einfluss)\n",
    "- `HIGH` → 3 (Hoher Einfluss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process news data using utility function\n",
    "final_df, stats_df = load_and_process_news_data(\n",
    "    csv_folder=CSV_FOLDER,\n",
    "    impact_mapping=IMPACT_MAPPING,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Display processing statistics\n",
    "print(\"\\nProcessing statistics:\")\n",
    "display(stats_df)\n",
    "\n",
    "print(\"\\nFirst 5 rows of the master DataFrame:\")\n",
    "display(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## **3. Zeitstempel-Rundung und Datenaggregation**\n",
    "\n",
    "### Warum Zeitstempel-Rundung?\n",
    "Da die News-Events zu verschiedenen exakten Zeiten stattfinden, runden wir alle Timestamps auf die **nächste halbe Stunde** auf. Dies ermöglicht:\n",
    "- **Konsistente Zeitintervalle** für die Analyse\n",
    "- **Gruppierung** mehrerer Events im gleichen Zeitfenster\n",
    "- **Bessere Korrelation** mit Chart-Daten (die ebenfalls in 30-Minuten-Intervallen vorliegen)\n",
    "\n",
    "### Rundungslogik\n",
    "- **Exakt 00:00 oder 30:00**: Keine Änderung\n",
    "- **00:01 - 29:59**: Aufrunden auf 30:00\n",
    "- **30:01 - 59:59**: Aufrunden auf nächste volle Stunde\n",
    "\n",
    "### Aggregierte Metriken\n",
    "\n",
    "#### Basis-Metriken (pro Zeitstempel)\n",
    "- **Event Count**: Anzahl aller News-Events\n",
    "- **Impact Statistiken**: Summe, Mittelwert, Maximum, Minimum, Standardabweichung\n",
    "- **Impact Verteilung**: Anzahl Events pro Impact-Level (0-3)\n",
    "- **Impact Diversität**: Anzahl unterschiedlicher Impact-Level\n",
    "\n",
    "#### Kategorie-spezifische Metriken\n",
    "Für jede News-Kategorie separat:\n",
    "- **Event Count**: Anzahl Events pro Kategorie\n",
    "- **Impact Sum**: Gesamter Impact pro Kategorie  \n",
    "- **Impact Max**: Höchster Impact pro Kategorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate news data using utility function\n",
    "agg = aggregate_news_data(final_df, verbose=True)\n",
    "\n",
    "# Overview of created features\n",
    "print(\"\\nÜbersicht der erstellten Features:\")\n",
    "feature_groups = {\n",
    "    'Basis-Metriken': [col for col in agg.columns if not col.startswith('cat_') and col != 'Timestamp'],\n",
    "    'Kategorie-Metriken': [col for col in agg.columns if col.startswith('cat_')]\n",
    "}\n",
    "\n",
    "for group, features in feature_groups.items():\n",
    "    print(f\"\\n{group} ({len(features)} Features):\")\n",
    "    for feature in features:\n",
    "        print(f\"  • {feature}\")\n",
    "\n",
    "print(\"\\nErste 5 Zeilen der aggregierten Daten:\")\n",
    "display(agg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## **4. Datenqualitätsprüfung**\n",
    "\n",
    "### Überprüfung auf Null-Spalten\n",
    "Identifikation von Spalten, die ausschließlich den Wert 0 enthalten. Dies hilft bei:\n",
    "\n",
    "- **Data Cleaning**: Entfernung redundanter Features\n",
    "- **Feature Selection**: Fokus auf informative Variablen  \n",
    "- **Speicheroptimierung**: Reduzierung der Dateigröße\n",
    "- **Model Performance**: Vermeidung uninformativer Features\n",
    "\n",
    "### Warum entstehen Null-Spalten?\n",
    "- **Seltene Kategorien**: Manche News-Kategorien haben möglicherweise keine Events in bestimmten Zeiträumen\n",
    "- **Aggregation**: Durch die Zeitstempel-Rundung können manche Kombinationen leer werden\n",
    "- **Datenqualität**: Fehlende oder unvollständige Quelldaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze column quality using utility function\n",
    "only_zero_cols, column_stats = analyze_column_quality(agg, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## **5. Integration mit Chart-Daten und Finalisierung**\n",
    "\n",
    "### Ziel der Integration\n",
    "Zusammenführung der aggregierten News-Metriken mit den Finanzchart-Daten zur Erstellung eines **Master-Datasets** für Machine Learning Analysen.\n",
    "\n",
    "### Chart-Daten Verarbeitung\n",
    "1. **OHLC-Daten**: Open, High, Low, Close Preise\n",
    "2. **Return-Berechnung**: Prozentuale Preisveränderung basierend auf größter Bewegung\n",
    "3. **Volumen**: Handelsvolumen hinzufügen\n",
    "\n",
    "### Return-Berechnung Logik\n",
    "```\n",
    "Return = (Größte_Bewegung - Open) / Open * 100\n",
    "\n",
    "Größte_Bewegung = {\n",
    "    Low,   falls |Open - Low| > |High - Open|\n",
    "    High,  sonst\n",
    "}\n",
    "```\n",
    "\n",
    "### Zeitzone-Anpassung\n",
    "- **Problem**: Chart-Daten in UTC, US-Märkte in Eastern Time\n",
    "- **Lösung**: Automatische Daylight Saving Time (DST) Erkennung und Anpassung\n",
    "- **Ergebnis**: Synchronisierte Zeitstempel für korrekte Korrelation\n",
    "\n",
    "### Data Merge Strategie\n",
    "- **Join-Typ**: Left Join (Chart-Daten als Basis)\n",
    "- **Join-Key**: Timestamp (nach Rundung und Zeitzone-Anpassung)\n",
    "- **Missing Values**: Mit 0 auffüllen (= keine News-Aktivität)\n",
    "\n",
    "### Finale Filterung\n",
    "- **Wochenenden entfernen**: Sonntags-Daten ausschließen (Markt geschlossen)\n",
    "- **Nur Handelszeiten**: Fokus auf relevante Zeitperioden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Process chart data and merge with news data\n",
    "print(\"Starte finale Datenverarbeitung...\")\n",
    "\n",
    "# Load and process chart data\n",
    "df_chart = process_chart_data(CHART_DATA_PATH, verbose=True)\n",
    "\n",
    "# Merge all data and apply final processing\n",
    "df_merged = merge_and_finalize_data(\n",
    "    df_chart=df_chart,\n",
    "    df_news=agg,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Print comprehensive summary\n",
    "print_final_summary(df_merged, verbose=True)\n",
    "\n",
    "print(\"\\nErste 5 Zeilen des finalen Datasets:\")\n",
    "display(df_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## **6. Datenvorbereitung Abgeschlossen**\n",
    "\n",
    "### Was wurde erreicht?\n",
    "\n",
    "Das Notebook hat erfolgreich ein **Machine Learning-ready Dataset** erstellt durch:\n",
    "\n",
    "1. **Datenintegration**: 6 News-Kategorien + Chart-Daten zusammengeführt\n",
    "2. **Zeitharmonisierung**: Timestamps auf 30-Minuten-Intervalle standardisiert  \n",
    "3. **Feature Engineering**: 40+ quantitative Features aus News-Events generiert\n",
    "4. **Zeitzone-Korrektheit**: UTC/Eastern Time Synchronisation mit DST-Behandlung\n",
    "5. **Datenbereinigung**: Wochenend-Filterung und Qualitätskontrolle\n",
    "\n",
    "### Dataset-Übersicht\n",
    "\n",
    "| Aspekt | Details |\n",
    "|--------|---------|\n",
    "| **Format** | CSV-Datei, ML-ready |\n",
    "| **Größe** | ~2MB, optimiert für Analyse |\n",
    "| **Zeitraum** | Mehrere Monate Finanzdaten |\n",
    "| **Frequenz** | 30-Minuten Intervalle |\n",
    "| **Features** | 6 Chart + 40+ News Features |\n",
    "| **Target** | `Return` (prozentuale Preisveränderung) |\n",
    "\n",
    "\n",
    "### Output\n",
    "**Hauptdatei**: `data/merged/merged_by_timestamp.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
